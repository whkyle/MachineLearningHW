{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning and Statistics for Physicists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default score function used by sklearn to evaluate how well a regression model predicts data is the [coefficient of determination](https://en.wikipedia.org/wiki/Coefficient_of_determination) $R^2$. Implement the function below to calculate $R^2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "454c4875daf4ea02b91678ba27b353b6",
     "grade": false,
     "grade_id": "cell-11235479a624a1eb",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def calculate_R2(y_data, y_pred):\n",
    "    \"\"\"Calculate the coefficient of determination R2 for two arrays.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_data : array\n",
    "        Array of data values, must have the same shape as y_pred.\n",
    "    y_pred : array\n",
    "        Array of predicted values, must have the same shape as y_data.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Calculated coefficient of determination R2.\n",
    "    \"\"\"\n",
    "    assert y_data.shape == y_pred.shape\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "20775b902bfa609ee7bd619648060337",
     "grade": true,
     "grade_id": "cell-98a3e782013f56f8",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# A correct solution should pass the tests below.\n",
    "gen = np.random.RandomState(seed=123)\n",
    "N = 100\n",
    "x = gen.uniform(size=N)\n",
    "y_pred = 2 * x - 1\n",
    "y_data = y_pred + gen.normal(scale=0.1, size=N)\n",
    "assert np.round(calculate_R2(y_data, y_pred), 3) == 0.961\n",
    "assert np.round(calculate_R2(y_data, -y_pred), 3) == -2.935\n",
    "assert np.round(calculate_R2(y_pred, y_pred), 3) == 1.000\n",
    "assert np.round(calculate_R2(y_pred, -y_pred), 3) == -3.000\n",
    "# Check that predicting the mean has R2 = 0\n",
    "assert np.round(calculate_R2(y_data, np.full(N, np.mean(y_pred))), 3) == 0.000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will implement the [backpropagation](https://en.wikipedia.org/wiki/Backpropagation) technique for automatically calculating the gradients needed for efficient optimization in machine learning.  You would normally use a framework like TensorFlow or PyTorch for these calculations, but the purpose of this exercise is to gain some insight into how the magic happens.\n",
    "\n",
    "Any mathematical function can be represented as a computational graph. For example,\n",
    "$$\n",
    "F(x) = 1 + 2 x\n",
    "$$\n",
    "could be represented by a graph with input nodes $(1, 2, x)$, two internal nodes for the operations $(+, \\times)$, and an output node $F$.\n",
    "\n",
    "Working at a higher level of abstraction, a function:\n",
    "$$\n",
    "F(x, y, z) = f\\bigl(x, g(h(y), z)\\bigr)\n",
    "$$\n",
    "has input nodes $(x, y, z)$, internal nodes $(f, g, h)$ and an output node $F$. In general, the information flowing between nodes can be aribtrary tensors.\n",
    "\n",
    "An important feature of the graph representation is that the calculations required to flow the inputs $(x, y, z)$ forward to the output $y$ are naturally decomposed into localized steps associated with each node.  For example, the $g$ node can calculate its output $g(h(y))$ knowing only the value of its input $h(y)$ and not caring how this input was obtained or where its ouptut will flow.\n",
    "\n",
    "The key insight of the [backpropagation algorithm](https://en.wikipedia.org/wiki/Backpropagation) is that the calculation of partial derivatives can also be localized using the chain rule. For example, the $h$ node can propagate partial derivatives of the output $y$ back to its input $y$ using\n",
    "$$\n",
    "\\frac{\\partial F}{\\partial y}(x, y, z) = \\frac{\\partial F}{\\partial h}(x, z) \\frac{\\partial h}{\\partial y}(y)\n",
    "$$\n",
    "knowing only the value of $\\frac{\\partial F}{\\partial h}(x, z)$ and not caring how this was obtained, or how $\\frac{\\partial F}{\\partial y}$ will be used. Note however that this calculation flows **backwards** through the graph, since it requires the value of $\\frac{\\partial F}{\\partial h}(x, z)$ calculated by its output node $g$.\n",
    "\n",
    "When $F$, $y$ and $h$ are arbitrary tensors with (flattened) indices $\\mu$, $\\nu$ and $\\alpha$, respectively, the corresponding tensor chain rule is:\n",
    "$$\n",
    "\\frac{\\partial F_\\mu}{\\partial y_\\nu}(x, y, z) = \\sum_\\alpha \\frac{\\partial F_\\mu}{\\partial h_\\alpha}(x, z) \\frac{\\partial h_\\alpha}{\\partial y_\\nu}(y)\n",
    "$$\n",
    "Note that in both the forward and backward directions, we only ever do calculations with (tensors) of floating-point numbers, not abstract variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a base class to represent an arbitrary graph node, we can implement almost all of the logic required for forward and backward passes through a graph in a completely generic way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphNode(object):\n",
    "    \"\"\"A generic graph node for forward and backward compuation.\n",
    "    \"\"\"\n",
    "    def __init__(self, **input_nodes):\n",
    "        \"\"\"Initialize a generic graph node.\n",
    "        \"\"\"\n",
    "        self.name = self.__class__.__name__\n",
    "        self.input_nodes = input_nodes\n",
    "        self.input_grads = {inp: None for inp in self.input_nodes}\n",
    "        # Check that our input node names match the args defined for compute().\n",
    "        compute_args = set(inspect.signature(self.compute).parameters)\n",
    "        if set(input_nodes) != compute_args:\n",
    "            raise ValueError(\n",
    "                f'Input names {set(input_nodes)} do not match compute args {compute_args}.')\n",
    "        \n",
    "    def forward(self, verbose=False):\n",
    "        \"\"\"Perform a forward pass through the graph, flowing input values to output values.\n",
    "        \"\"\"\n",
    "        input_values = {inp: node.forward(verbose) for inp, node in self.input_nodes.items()}\n",
    "        self.input_shapes = {inp: value.shape for inp, value in input_values.items()}\n",
    "        self.value = self.compute(**input_values)\n",
    "        if verbose:\n",
    "            print(self.name, 'forward:', input_values, '->', self.value)\n",
    "        return self.value\n",
    "    \n",
    "    def backward(self, verbose=False, grad_in=None):\n",
    "        \"\"\"Perform a backward pass through the graph, flowing output gradients to input gradients.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'value'):\n",
    "            raise ValueError('Must call forward() before backward().')\n",
    "        if grad_in is None:\n",
    "            grad_in = np.ones(self.value.shape)\n",
    "        elif grad_in.shape != self.value.shape:\n",
    "            raise ValueError(f'Input grad has shape {grad_in.shape} but expected {self.value.shape}.')\n",
    "        final_shape = tuple(grad_in.shape[:-len(self.value.shape)])\n",
    "        for inp, node in self.input_nodes.items():\n",
    "            expected_shape = self.value.shape + self.input_shapes[inp]\n",
    "            if self.input_grads[inp].shape != expected_shape:\n",
    "                raise ValueError(f'{self.name} grad wrt \"{inp}\" should have shape {expected_shape}.')\n",
    "            grad_out = grad_in.reshape(-1, self.value.size).dot(self.input_grads[inp].reshape(self.value.size, -1))\n",
    "            grad_out = grad_out.reshape(final_shape + self.input_shapes[inp])\n",
    "            if verbose:\n",
    "                print(self.name, 'backward:', grad_out, '<-', grad_in)\n",
    "            node.backward(verbose=verbose, grad_in=grad_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The leaf nodes are a specialized graph node that feed a tensor into the graph (to propagate forward) and store the corresponding gradients wrt the graph output (from back propagation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor(GraphNode):\n",
    "    \n",
    "    def __init__(self, value):\n",
    "        self.value = np.asarray(value)\n",
    "        \n",
    "    def forward(self, verbose=False):\n",
    "        return self.value\n",
    "        \n",
    "    def backward(self, grad_in, verbose=False):\n",
    "        self.grad = grad_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All non-leaf graph nodes inherit from `GraphNode` and implement a `compute` method that:\n",
    " - Specifies (arbitrary) variable names for each of its inputs.\n",
    " - Computes and stores the backward partial derivatives wrt to each of its inputs.\n",
    " - Computes and returns the forward value.\n",
    "\n",
    "For example, the following function implements $\\sin(x)$ element-wise on an arbitrary input 1D tensor (aka vector) $x$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ELEMENTWISE = lambda z: np.diag(z.reshape(-1)).reshape(z.shape + z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sin(GraphNode):\n",
    "    \n",
    "    def compute(self, x):\n",
    "        self.input_grads['x'] = ELEMENTWISE(np.cos(x))\n",
    "        return np.sin(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the use of `ELEMENTWISE` to calculate the full gradient matrix\n",
    "$$\n",
    "\\frac{\\partial \\sin x_\\mu}{\\partial x_\\nu} \\; .\n",
    "$$\n",
    "from its diagonal elements, which is a tensor generalization of the [np.diag](https://docs.scipy.org/doc/numpy/reference/generated/numpy.diag.html) function.\n",
    "\n",
    "Build a simple graph to see the forward and backward flows in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = Tensor(np.linspace(0, 2 * np.pi, 25))\n",
    "sin_theta = Sin(x=theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sin_theta.forward()\n",
    "plt.plot(theta.value, sin_theta.value, '.');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sin_theta.backward()\n",
    "plt.plot(theta.value, theta.grad, '.');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the compute method below to compute the element-wise [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function):\n",
    "$$\n",
    "f(s) = \\frac{1}{1 + \\exp(-s)}\n",
    "$$\n",
    "for an arbitrary input vector $s$. This node is a popular choice of \"activation function\" to introduce some non-linearity into a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "6d18945151be41a7d8161693ac3d0660",
     "grade": false,
     "grade_id": "cell-174fa43b2c484536",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class Sigmoid(GraphNode):\n",
    "    \n",
    "    def compute(self, s):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b277fb8f71e6e08986a24595e87c2d1b",
     "grade": true,
     "grade_id": "cell-c3a629dece534703",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# A correct solution should pass the tests below.\n",
    "s = Tensor([-1., 0., +1.])\n",
    "f = Sigmoid(s=s)\n",
    "f.forward()\n",
    "assert np.allclose(f.value, [0.26894142, 0.5, 0.73105858])\n",
    "assert f.input_grads['s'].shape == (3,3)\n",
    "f.backward()\n",
    "assert np.allclose(s.grad, f.value * (1 - f.value))\n",
    "z = Tensor(np.zeros(10))\n",
    "f = Sigmoid(s=z)\n",
    "f.forward()\n",
    "assert f.value.shape == (10,)\n",
    "f.backward()\n",
    "assert z.grad.shape == (10,)\n",
    "s = Tensor([[-1.], [0.], [+1.]])\n",
    "f = Sigmoid(s=s)\n",
    "f.forward()\n",
    "assert np.allclose(f.value, [[0.26894142], [0.5], [0.73105858]])\n",
    "assert f.input_grads['s'].shape == (3, 1, 3, 1)\n",
    "f.backward()\n",
    "assert np.allclose(s.grad, f.value * (1 - f.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the compute method below to compute the linear transformation:\n",
    "$$\n",
    "Y(X) = X W + b\n",
    "$$\n",
    "where $W$ is an $D\\times M$ \"weight\" matrix, $X$ is a $N\\times D$ data matrix ($N$ samples and $D$ features), and $b$ is a vector of length $M$ that is added to each row (i.e., numpy broadcast style). This node is a key building block for a (fully connected) neural network.\n",
    "\n",
    "Hint: the resulting tensor $Y(X)$ has dimensions $N\\times M$, so the gradients with respect to $X$ and $W$ have four dimensions.  For example,\n",
    "$$\n",
    "\\frac{\\partial Y_{nm}}{\\partial X_{ab}} = \\delta_{na} W_{mb} \\; ,\n",
    "$$\n",
    "and this gradient tensor has shape (N, M, N, D)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7b210c03776b536c6edca794916af116",
     "grade": false,
     "grade_id": "cell-1ed2f4c3a7af70b9",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class Linear(GraphNode):\n",
    "    \n",
    "    def compute(self, X, W, b):\n",
    "        assert len(W.shape) == 2 and len(X.shape) == 2 and len(b.shape) == 1\n",
    "        M = len(b)\n",
    "        N, D = X.shape\n",
    "        assert W.shape == (D, M)\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fcb4d1d994d66a7db7f92ac6f8169b6b",
     "grade": true,
     "grade_id": "cell-2e1b085eb2e1a6c6",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# A correct solution should pass the tests below.\n",
    "X = Tensor([[-1, 0, 1]])\n",
    "W = Tensor(np.identity(3))\n",
    "b = Tensor(np.zeros(3))\n",
    "Y = Linear(X=X, W=W, b=b)\n",
    "Y.forward()\n",
    "assert np.array_equal(Y.value, X.value)\n",
    "assert Y.input_grads['X'].shape == (1, 3, 1, 3)\n",
    "assert Y.input_grads['W'].shape == (1, 3, 3, 3)\n",
    "assert Y.input_grads['b'].shape == (1, 3, 3)\n",
    "Y.backward()\n",
    "assert np.array_equal(X.grad, [[1., 1., 1.]])\n",
    "assert np.array_equal(b.grad, [1., 1., 1.])\n",
    "assert np.array_equal(W.grad, np.tile([-1., 0., 1.], (3, 1)).T)\n",
    "\n",
    "W = Tensor(np.arange(12).reshape(3, 4))\n",
    "X = Tensor(np.ones((2, 3)))\n",
    "b = Tensor(np.arange(4))\n",
    "Y = Linear(X=X, W=W, b=b)\n",
    "Y.forward()\n",
    "assert np.array_equal(Y.value, np.tile([12., 16., 20., 24.], (2, 1)))\n",
    "Y.backward()\n",
    "assert np.array_equal(W.grad, np.full((3, 4), 2.))\n",
    "assert np.array_equal(b.grad, np.full(4, 2.))\n",
    "assert np.array_equal(X.grad, np.tile([6., 22., 38.], (2, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the compute method below to calculate the mean squared difference between arbitrary (but identically shaped) tensors $x$ and $y$,\n",
    "$$\n",
    "\\text{MSE}(x, y) = \\frac{1}{N} \\sum_\\mu (x_\\mu - y_\\mu)^2 \\; ,\n",
    "$$\n",
    "also known as the \"mean-squared error (MSE) loss\" function when the two vectors are target and predicted values, and related to the \"L2 loss\". Note that the result is always a 1D tensor of length 1, independent of the shape of $x$ and $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "fbb8e60d2c61fe499f572d870c925398",
     "grade": false,
     "grade_id": "cell-4633dd16e1dd1023",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class MSELoss(GraphNode):\n",
    "    \n",
    "    def compute(self, target, predicted):\n",
    "        assert target.shape == predicted.shape\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "67f12cb59b5d69d906f4862067eed04d",
     "grade": true,
     "grade_id": "cell-21e885b24080bbac",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# A correct solution should pass the tests below.\n",
    "x = Tensor(np.ones(3))\n",
    "y = Tensor(np.zeros(3))\n",
    "loss = MSELoss(target=x, predicted=y)\n",
    "loss.forward()\n",
    "assert loss.value == 1.\n",
    "loss.backward()\n",
    "assert np.allclose(x.grad, +2. / 3.)\n",
    "assert np.allclose(y.grad, -2. / 3.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the building blocks necessary to create a (fully connected) neural network.  Implement the function below to build the graph for a neural network with a single \"hidden\" (internal) layer. The corresponding function is:\n",
    "$$\n",
    "Y(X) = \\varphi(X W_1 + b_1)\\,W_2 + b_2 \\; ,\n",
    "$$\n",
    "where $\\varphi(s)$ is the (element-wise) sigmoid function and $X$ is a dataset of input values represented by a $N\\times D$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f7e7648473ae2b373414f9c4960af7ed",
     "grade": false,
     "grade_id": "cell-99fdd71d71081f28",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def build_network(X, W1, b1, W2, b2):\n",
    "    \"\"\"Build a neural network with one hidden layer.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X, W1, b1, W2, b2: Tensor objects of compatible shapes.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Graph represented by a GraphNode object.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "06306e2cad9654d8690955cdf220174e",
     "grade": true,
     "grade_id": "cell-bd1ee533cae69360",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# A correct solution should pass the tests below.\n",
    "N, D, M = 4, 3, 2\n",
    "X = Tensor(np.ones((N, D)))\n",
    "W1 = Tensor(np.ones((D, M)))\n",
    "b1 = Tensor(np.zeros(M))\n",
    "W2 = Tensor(np.ones((M, 1)))\n",
    "b2 = Tensor([0.])\n",
    "Y = build_network(X, W1, b1, W2, b2)\n",
    "Y.forward()\n",
    "assert np.allclose(Y.value, 1.90514825)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use the code developed so far to train a neural network consisting of one input node, 25 hidden nodes, and one output node. First set up the input dataset $X$ and corresponding target output values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input values of theta on a [0,2pi] grid.\n",
    "n_input = 100\n",
    "theta = Tensor(np.linspace(0, 2 * np.pi, n_input).reshape(n_input, 1))\n",
    "sin_theta = Sin(x=theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next declare the model parameters, with values initialized from a unit Gaussian distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a neural network graph with 1 hidden layer.\n",
    "n_hidden = 25\n",
    "gen = np.random.RandomState(seed=123)\n",
    "W1 = Tensor(gen.normal(size=(1, n_hidden)))\n",
    "b1 = Tensor(gen.normal(size=n_hidden))\n",
    "W2 = Tensor(gen.normal(size=(n_hidden, 1)))\n",
    "b2 = Tensor(gen.normal(size=(1,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, build the model and define the loss metric we will minimize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_network(theta, W1, b1, W2, b2)\n",
    "loss = MSELoss(target=sin_theta, predicted=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now \"learn\" to transform $\\theta$ into $\\sin(\\theta)$ using the stochastic gradient descent (SGD) algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(learning_rate=0.05, n_epochs=20):\n",
    "    losses = []\n",
    "    for epoch in range(n_epochs):\n",
    "        losses.append(loss.forward())\n",
    "        loss.backward()\n",
    "        W2.value -= learning_rate * W2.grad\n",
    "        W1.value -= learning_rate * W1.grad\n",
    "        b1.value -= learning_rate * b1.grad\n",
    "        b2.value -= learning_rate * b2.grad\n",
    "    plt.plot(losses, 'o-')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Training Epoch')\n",
    "    plt.ylabel('MSE loss')\n",
    "    \n",
    "learn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the [redshift case study](https://nbviewer.jupyter.org/github/dkirkby/MachineLearningStatistics/blob/master/notebooks/Redshift.ipynb) as a template for your own case study using your adopted dataset...\n",
    "\n",
    "This is an open ended assignment, but here are some suggested studies:\n",
    " - Define your baseline supervised learning problem. Is it a regression or classification problem?\n",
    " - Find a good value of `n_neighbors` for a KNN solution to your problem.\n",
    " - Find a good value of `max_depth` for a Decision Tree solution to your problem.\n",
    " - What is the most important variable according to the optimum decision tree?\n",
    " - Quantify the improvement when using a random forest instead of a single tree."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
